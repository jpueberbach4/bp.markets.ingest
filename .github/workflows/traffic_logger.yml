name: "Daily Traffic Logger"

on:
  schedule:
    # Changed to 23:00 UTC to ensure 'today's' data is processed by GitHub API
    - cron: '0 23 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-stats:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.TRAFFIC_ACTION_TOKEN }}
          fetch-depth: 0

      - name: Fetch Traffic Data
        uses: sangonzal/repository-traffic-action@v.0.1.6
        env:
          TRAFFIC_ACTION_TOKEN: ${{ secrets.TRAFFIC_ACTION_TOKEN }}

      - name: Fix Permissions
        run: |
          sudo chown -R $USER:$USER .
          sudo chmod -R 777 traffic/ || true

      - name: Merge Stats
        run: |
          # 1. Debug: Show what the API actually returned for today
          echo "Checking new traffic data..."
          [ -f traffic/views.csv ] && tail -n 5 traffic/views.csv || echo "No views.csv found"
          
          # 2. Sync Repo State
          mv traffic /tmp/traffic_new
          git fetch origin main 
          git reset --hard origin/main
          git clean -fd
          mv /tmp/traffic_new traffic

          mkdir -p .github/traffic
          cat << 'EOF' > merge_stats.py
          import csv
          import os

          def merge_csv(name):
              new_path = f'traffic/{name}.csv'
              old_path = f'.github/traffic/{name}.csv'
              if not os.path.exists(new_path): return
              
              data = {}
              fieldnames = []

              def get_id_col(reader):
                  return reader.fieldnames[0] if reader.fieldnames else None

              if os.path.exists(old_path) and os.path.getsize(old_path) > 0:
                  with open(old_path, 'r', newline='') as f:
                      reader = csv.DictReader(f)
                      id_col = get_id_col(reader)
                      if id_col:
                          fieldnames = reader.fieldnames
                          for row in reader:
                              if row.get(id_col):
                                  data[row[id_col]] = row
              
              with open(new_path, 'r', newline='') as f:
                  reader = csv.DictReader(f)
                  id_col = get_id_col(reader)
                  if id_col:
                      # Update fieldnames in case GitHub adds columns
                      fieldnames = reader.fieldnames 
                      for row in reader:
                          if row.get(id_col):
                              # This ensures today's data overwrites or appends correctly
                              data[row[id_col]] = row
              
              if not data or not fieldnames: return

              id_col = fieldnames[0]
              # Sort ensures chronological order
              sorted_keys = sorted(data.keys())
              with open(old_path, 'w', newline='') as f:
                  writer = csv.DictWriter(f, fieldnames=fieldnames)
                  writer.writeheader()
                  for k in sorted_keys:
                      writer.writerow(data[k])

          merge_csv('clones')
          merge_csv('views')
          EOF
          
          python3 merge_stats.py

          # 3. Handle Plots and Cleanup
          cp -f traffic/*.png .github/traffic/ 2>/dev/null || true
          
          # Removed manual 'git add' to let the 'add-and-commit' action handle it safely
          sudo rm -rf traffic merge_stats.py

      - name: Commit and Push
        uses: Endbug/add-and-commit@v9
        with:
          author_name: "GitHub Action"
          author_email: "action@github.com"
          message: "Update stats [skip ci]"
          add: ".github/traffic/*"
          pull: '--rebase --autostash -X theirs'
          push: true